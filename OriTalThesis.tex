%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Masters/Doctoral Thesis 
% LaTeX Template
% Version 2.4 (22/11/16)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Version 2.x major modifications by:
% Vel (vel@latextemplates.com)
%
% This template is based on a template by:
% Steve Gunn (http://users.ecs.soton.ac.uk/srg/softwaretools/document/templates/)
% Sunil Patel (http://www.sunilpatel.co.uk/thesis-template/)
%
% Template license:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[
12pt, % The default document font size, options: 10pt, 11pt, 12pt
%oneside, % Two side (alternating margins) for binding by default, uncomment to switch to one side
english, % ngerman for German
doublespacing, % Single line spacing, alternatives: singlespacing, onehalfspacing or doublespacing
%draft, % Uncomment to enable draft mode (no pictures, no links, overfull hboxes indicated)
%nolistspacing, % If the document is onehalfspacing or doublespacing, uncomment this to set spacing in lists to single
%liststotoc, % Uncomment to add the list of figures/tables/etc to the table of contents
%toctotoc, % Uncomment to add the main table of contents to the table of contents
%parskip, % Uncomment to add space between paragraphs
%nohyperref, % Uncomment to not load the hyperref package
headsepline, % Uncomment to get a line under the header
%chapterinoneline, % Uncomment to place the chapter title next to the number on one line
%consistentlayout, % Uncomment to change the layout of the declaration, abstract and acknowledgements pages to match the default layout
]{MastersDoctoralThesis} % The class file specifying the document structure

\usepackage[utf8]{inputenc} % Required for inputting international characters
\usepackage[T1]{fontenc} % Output font encoding for international characters

\usepackage{palatino} % Use the Palatino font by default
%\usepackage{mathptmx}
\usepackage[square,numbers]{natbib}
\usepackage{multicol,graphicx}
\usepackage{subcaption}
\usepackage{placeins}
\usepackage{booktabs}
\usepackage{titlesec}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{filecontents}
\usepackage{placeins}
\usepackage[export]{adjustbox}
\usepackage{makecell}
\usepackage{caption}
\usepackage{color}
\usepackage{float}
\usepackage{amsmath}
\usepackage{tabularx}
\usepackage{scrextend}

\usepackage[autostyle=true]{csquotes}
\graphicspath{ {Figures/} }
\captionsetup[table]{position=bottom} 
%----------------------------------------------------------------------------------------
%	MARGIN SETTINGS
%----------------------------------------------------------------------------------------

\geometry{
	paper=a4paper, % Change to letterpaper for US letter
	inner=2.5cm, % Inner margin
	outer=3.8cm, % Outer margin
	bindingoffset=.5cm, % Binding offset
	top=1.5cm, % Top margin
	bottom=1.5cm, % Bottom margin
	%showframe, % Uncomment to show how the type block is set on the page
}

%----------------------------------------------------------------------------------------
%	THESIS INFORMATION
%----------------------------------------------------------------------------------------

\thesistitle{Using Recurrent Neural Networks for P300-based BCI} % Your thesis title, this is used in the title and abstract, print it elsewhere with \ttitle
\advisor{Dr. Doron \textsc{Friedman}} % Your supervisor's name, this is used in the title page, print it elsewhere with \supname
\examiner{} % Your examiner's name, this is not currently used anywhere in the template, print it elsewhere with \examname
\degree{Master of Science (M.Sc.) Research Track
	in Computer Science} % Your degree name, this is used in the title page and abstract, print it elsewhere with \degreename
\author{Ori \textsc{Tal}} % Your name, this is used in the title page and abstract, print it elsewhere with \authorname
\addresses{} % Your address, this is not currently used anywhere in the template, print it elsewhere with \addressname

\subject{Biological Sciences} % Your subject area, this is not currently used anywhere in the template, print it elsewhere with \subjectname
\keywords{} % Keywords for your thesis, this is not currently used anywhere in the template, print it elsewhere with \keywordnames
\university{\href{http://www.university.com}{The Interdisciplinary Center, Herzlia}} % Your university's name and URL, this is used in the title page and abstract, print it elsewhere with \univname
\department{\href{http://department.university.com}{Computer science}} % Your department's name and URL, this is used in the title page and abstract, print it elsewhere with \deptname
\group{\href{http://researchgroup.university.com}{Research Group Name}} % Your research group's name and URL, this is used in the title page, print it elsewhere with \groupname
\faculty{\href{https://www.idc.ac.il/he/schools/cs/pages/home.aspx}{Efi Arazi School of Computer Science}} % Your faculty's name and URL, this is used in the title page and abstract, print it elsewhere with \facname

\AtBeginDocument{
\hypersetup{pdftitle=\ttitle} % Set the PDF's title to your title
\hypersetup{pdfauthor=\authorname} % Set the PDF's author to your name
\hypersetup{pdfkeywords=\keywordnames} % Set the PDF's keywords to your keywords
}

\begin{document}

\frontmatter % Use roman page numbering style (i, ii, iii, iv...) for the pre-content pages

\pagestyle{plain} % Default to the plain heading style until the thesis style is called for the body content

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\begin{titlepage}
\begin{center}

\vspace*{.06\textheight}
{\scshape\LARGE \univname\par}\vspace{0.5cm} % University name
\textsc{\Large Master Thesis}\\[0.1cm] % Thesis type

\HRule \\[0.1cm] % Horizontal line
{\huge \bfseries \ttitle\par}\vspace{0.1cm} % Thesis title
\HRule \\[0.1cm] % Horizontal line
 
\begin{minipage}[t]{0.4\textwidth}
\begin{flushleft} \large
\emph{Author:}\\
\href{http://www.johnsmith.com}{\authorname} % Author name - remove the \href bracket to remove the link
\end{flushleft}
\end{minipage}
\begin{minipage}[t]{0.4\textwidth}
\begin{flushright} \large
\emph{Advisor:} \\
\href{http://www.jamessmith.com}{\supname} % Supervisor name - remove the \href bracket to remove the link  
\end{flushright}
\end{minipage}\\[1cm]


\large \textit{A thesis submitted in fulfillment of the requirements\\ for the degree of \degreename}\\[0.3cm] % University requirement text

 
\vfill

{\large \today}\\[4cm] % Date
%\includegraphics{Logo} % University/department logo - uncomment to place it
 
\vfill
\end{center}
\end{titlepage}

%----------------------------------------------------------------------------------------
%	ABSTRACT PAGE
%----------------------------------------------------------------------------------------
\begin{abstract}
\addchaptertocentry{\abstractname} P300-based spellers are one of the main methods for EEG-based brain computer interface, and the detection of the P300 target event with high accuracy is an important prerequisite. The rapid serial visual presentation (RSVP) protocol is of high interest because it can be used by patients who have lost control over their eyes. In this study we wish to explore the suitability of recurrent neural networks (RNNs) as a machine learning method for identifying the P300 signal in RSVP data. We systematically compare RNN with alternative methods such as linear discriminant analysis (LDA) and convolutional neural network (CNN). Our results indicate that RNN shows good results only with large amounts of data, and we show that a network combining CNN and RNN is significantly more resilient to temporal noise than other methods.
\end{abstract}
%----------------------------------------------------------------------------------------
%	ACKNOWLEDGEMENTS
%----------------------------------------------------------------------------------------
\begin{acknowledgements}
\addchaptertocentry{\acknowledgementname} 

I wish to express my sincere thanks to Dr.Doron Friedman who gave me the freedom to explore the new field of deep learning and supported me along the way.

Special  thanks to my wife Shira, for her utmost support, patience and belief in my work.

\end{acknowledgements}

%----------------------------------------------------------------------------------------
%	LIST OF CONTENTS/FIGURES/TABLES PAGES
%----------------------------------------------------------------------------------------

\tableofcontents % Prints the main table of contents

%\listoffigures % Prints the list of figures

%\listoftables % Prints the list of tables

%	THESIS CONTENT - CHAPTERS
%----------------------------------------------------------------------------------------

\mainmatter % Begin numeric (1,2,3...) page numbering

\pagestyle{thesis} % Return the page headers back to the "thesis" style

% Include the chapters of the thesis as separate files from the Chapters folder
% Uncomment the lines as you write the chapters


\chapter{Introduction}

\vspace{0.4cm}


\textit{This thesis is based on an article presented in the 7th Graz Brain-Computer Interface Conference. While the article  is focused on describing our results, in this report we added a more detailed explanation about the method we were using in our experiments.}

Neural networks have recently been shown to achieve outstanding performance in several machine learning domains such as image recognition \cite{krizhevsky2012imagenet} and voice recognition ~\cite{hinton2012deep}. Most of these breakthroughs have been achieved with convolutional neural networks (CNNs)~\cite{Lenet98}, but some promising results has also been demonstrated by using recurrent neural networks (RNNs) for tasks such as speech and handwriting recognition~\cite{graves2013speech, graves2008unconstrained}, usually when using the long short-term memory (LSTM) architecture ~\cite{LSTM_origin}.

There have been some studies using 'deep neural networks' for P300 classification \cite{P300_CNN, RSVP_P300_geva}. The results reported, despite some success, do not show the same dramatic progress achieved by 'deep learning' methods as compared to the previous state of the art; while in areas such as image or voice recognition `deep' neural networks have resulted in classification accuracy exceeding other methods by far, this has not yet been the case with EEG in general and in P300 detection specifically. The small number of samples typically available in neuroscience (or brain computer interface - BCI) is most likely one of the main reasons for theses results. In addition, the high dimensionality of the EEG signal, the low signal to noise (SNR) and the existence of outliers in the data, pose other difficulties when trying to use neural networks for BCI tasks (see \cite{lotte2007review}). The main question in this research is whether the RNN model, and particularly LSTM, can enhance the accuracy of P300-based BCI systems and if so, under what conditions.

P300-based BCI systems rely on identifying the times when a subject is required to pay attention toward a rare event, by examining the subject's electroencephalogram (EEG) data. The first BCI system that used the P300 effect was presented by Farwell, and Donchin \cite{FirstP300} and since then different versions of P300 based BCI systems were suggested. One example of such a paradigm is the P300 rapid serial visual presentation (RSVP) speller. In this paradigm letters are presented one after the other in a random order, and the subject is asked to pay attention only to one of the letters called \textit{target} letter or \textit{target stimuli} (by counting them silently, for example). Whenever a subject pays attention to the target letter, a special waveform called P300 is expected to occur. It is called P300 since there is usually a peak in the EEG amplitude 300ms after the presentation of a target event. The advantage of the RSVP paradigm is that it does not require any eye movements, and can thus be operated by patients who have lost control of their eye gaze completely.

\section{Deep Neural Networks - Overview}


Deep neural network (DNN) means relatively large artificial neural network with multiple layers. There are two main types of ANN architectures: feed forward neural networks (FFNN or FF) and recurrent neural network (RNN). In FFNN directed cycles are not allowed (i.e., data can flow only to the next layer), while the RNN architecture allows directed cycles within the network (specifically, data can also flow between 'neurons' in the same layer). The directed cycles in RNN allows the network to 'remember' past events and making it suitable for sequence learning \cite{rumelhart1985learning}.

\subsection{Layer Types}
The architecture we propose is a combination of several ANN layer types described below:

\textbf{Fully connected layer - FC} - a layer where each neuron in the input is connected to each neuron in the output is often called fully connected layer or FC. In an FC layer, the output is obtained by the following equation:
\[y=\sigma \left( {xW + b} \right)\]

where $x$ is the input vector, $W$ is a matrix that represents a linear mapping and $b$ is a vector that reflects the transformation called the $bias$ ($b$ holds the value for each output unit). $\sigma \left( \cdot \right)$ represent an element-wise non-linearity such as rectified linear unit (ReLU), sigomid or hyperbolic tangent (TanH):

\[{\rm{ReLu}}\left( x \right) = \max \left( {x,0} \right) \,\,\,,\,\,\,{\rm{sigmoid}}\left( x \right) = \frac{1}{{1 + {e^{ - x}}}}\,\,\,,\,\,\,{\mathop{\rm Tan}\nolimits} {\rm{H}}\left( x \right) = \frac{{{e^x} - {e^{ - x}}}}{{{e^x} + {e^{ - z}}}}\]

Fig.\ref{fig:FF_Example} shows an example of a simple network with two inputs and outputs, and one fully connected layer with 3 cells and. All the nodes in the hidden layer are connected to all input and output nodes. Since it is a feed forward architecture, the data flows only in one direction: from the input, to the hidden layer, and then to the output.

\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{Figures/FF_Example_v2}
\caption{Illustration of a simple feed-forward network with one hidden layer. $h_{i,j}$ means the $i$-th hidden cell on the $j$-th layer. }
\label{fig:FF_Example}
\end{figure}

\textbf{Convolutional Neural Network Layer - CNN } CNN is a layer that utilizes the local correlation between adjacent cells of the input layer. Since, unlike the FC layer, the output can be of multiple dimensions, we refer to the output as \textit{feature map}. The feature maps are obtained by activating trainable multi-dimension kernels across the input layer. Fig.\ref{fig:CNN_Example} is an example of a 1D CNN. The red, blue and green colors represent the trainable kernel weights ($w1$, $w2$ and $w3$ respectively) across the input layer. Equation \ref{eq:1} describes the output of element $i$ in a 1D CNN layer feature map:
\begin{equation} \label{eq:1}
y\left( i \right) = \sigma \left( {W * x\left[ {i - k,...,i + k} \right] + b} \right)
\end{equation}
If the kernel is a 1D filter with a length of $k$ and the next layer has $M$ outputs, then $W$ is a matrix of size $M \times k$.

\begin{figure}[t]
			\centering
			\includegraphics{CNN_Example_v2.png}
			\caption[A Schematic diagram of a 1D CNN layer. The input number reflects the sample's order in time (i.e., "input 1" is the first input and "input 5" is the last input).]{A Schematic diagram of a 1D CNN layer. The input number reflects the sample's order in time (i.e., "input 1" is the first input and "input 5" is the last input).}
			\label{fig:CNN_Example}
\end{figure}

\textbf{Recurrent Neural Network Layer - RNN} - The simplest form of a recurrent neural network \cite{rumelhart1985learning, werbos1988generalization} is a layer where the output is connected to the input. Unlike feed forward layers, in RNN, $y[t]$ (the layer output at time $t$) is a function of both the current input $x[t]$ and $y[t-1]$ (the previous state): 
\begin{equation}
	y\left( t \right) = \sigma \left( {x\left[t \right]W + y\left[ {t - 1} \right]U + b} \right)
\end{equation}

Here, again $\sigma $ is a non-linear activation function, $W$ is the input weight matrix and $U$ is the previous results weight matrix. The structure of an RNN layer, allows the network to contain memory, since it has access to information from previous time-stamps within each predicted sample. RNN is known to suffer from phenomena called "vanishing gradient" and "exploding gradient" \cite{pascanu2013difficulty}: while training, the gradient of the loss function may not propagate to the first layers (i.e., layers closer to the input layer) or may have very large values (thus update the layer weight too much). These problems prevent learning long temporal dependencies (the full explanation is beyond the scope of this report see \cite{bengio1994learning}) .A common solution for these problem is called Long Short Term Memory layer.


\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{Figures/RNN_Example}
	\caption{A schematic diagram of a simple recurrent neural network with one hidden layer.}
	\label{fig:RNN_Example}
\end{figure}



\begin{figure}
	\centering
	\begin{subfigure}[t]{0.3\textwidth}
		\centering
		\includegraphics[height=5cm]{LSTM_RNN_placeholder_clear_vanilla_rnn.png}
		\caption{First}\label{fig:f}
	\end{subfigure}
	\begin{subfigure}[t]{0.3\textwidth}
		\centering
		\includegraphics[height=5cm]{LSTM_RNN_placeholder_clear_lstm.png}
		\caption{Second}\label{fig:s}
	\end{subfigure}
	\begin{subfigure}[t]{0.3\textwidth}
		\centering
		\includegraphics[height=5cm]{LSTM_RNN_placeholder_clear_legend.png}

	\end{subfigure}
	\caption{A schematic illustration of the Simple Recurrent Network unit (left) and a Long Short-Term Memory block (right) as used
		in the hidden layers of a recurrent neural network. Taken from \cite{ greff2017lstm}}
	\label{fig:LSTM_arch}
\end{figure}









\textbf{Long Short Term Memory Layer - LSTM} - LSTM is a type of RNN with a special architecture designed to overcome RNN's difficulties in learning long term dependencies. In addition to the output - $y[t]$, LSTM also has a memory cell - $c[t]$. LSTM uses an architecture with a set of "gates" that allow it to decide whether when a data should be stored in the memory unit, and by that, overcoming the vanishing and exploding gradients problem mentioned above. The right illustration in Fig.\ref{fig:LSTM_arch} and Eq.\ref{eq_lstm} describes the LSTM function:

${W_{\left\{ {i,p,f,o} \right\}}}$ represents weights of the current state and  ${U_{\left\{ {i,p,f,o} \right\}}}$ represents weights of the previous states. 


\chapter{Previous work}
There are many methods for building systems that can identify the P300 target for a BCI task. Blankertz et al.~\cite{P300_Tutorial} suggest 
selecting the time intervals with maximal separation between the target and non target samples, averaging their \textit{electro-potential} value and use shrinkage LDA to classify these features. Using this method has a drawback due to the low complexity of the LDA model \cite{cincotti2003comparison}. The winner of the BCI competition III: dataset II used an ensemble of support vector machines (SVM) \cite{P300SVMWinner}, and other methods include hidden Markov model, k-nearest neighbours, and more  \cite{cincotti2003comparison}.

More recently, given the success of deep neural networks \cite{krizhevsky2012imagenet}, there have been several attempts to apply `deep learning' for BCI related tasks. Cecotti and Graser~\cite{P300_CNN} were the first to use CNNs  for a P300 speller. In their work, they train an ensemble of CNN-based P300 classifiers to identify the existence of a P300 event. Manor and Geva~\cite{RSVP_P300_geva} used CNN for the RSVP P300 classification task and suggested a new spatio-temporal regularization which have shown improvement in the performance.


Unlike feed forward network models such as CNN and multi-layer perceptron (MLP),  the RNN architecture allows directed cycles within the network, which enable the model to ``memorize past events''. LSTM \cite{LSTM_origin} is a type of RNN, which includes a special node that can be described as a differentiable memory cell. The specific architecture of LSTM enables it to overcome some of the weakness of simple RNNs~\cite{bengio1994learning}.

There are several reasons why LSTM is a good candidate for modelling the P300 pattern. First, RNN and LSTM have shown success when modeling time series for tasks such as handwriting and speech recognition \cite{graves2013speech,  graves2008unconstrained, yue2015beyond}. Second, RNN is known to have the capability to approximate dynamical systems \cite{li2005approximation}, which makes it a natural candidate for modelling the dynamics of EEG data. Another motivation is that RNN can be seen as a powerful form of hidden Markov models (HMM), which have been shown to classify EEG successfully \cite{solhjoo2005classification,obermaier2001hidden,cincotti2003comparison}; RNNs can be seen as HMMs with an exponentially large state space and an extremely compact parametrization~\cite{sutskever2009recurrent}.

LSTM was already used for analysing EEG data for emotion detection \cite{soleymani2014continuous} and a phenomena called behavioral microsleeps \cite{davidson2005detecting}. Bahshivan et al.~\cite{LSTM_EEG} modeled inter-subject EEG features for identifying cognitive load by using convolutional LSTM. They created a video from three different band powers in each electrode. One of the major differences between their work and ours is that we use the original signal without any feature extraction (such as band power), and we focus specifically on the P300 speller domain.
\chapter{MATERIALS AND METHODS}

We compared the performance of LSTM based methods with other methods on a dataset from a RSVP P300 speller study~\cite{BlaknertzExperiment}. We used  average prediction across 10 trials to measure the P300 speller accuracy as applied in~\cite{BlaknertzExperiment}.

\section{P300 speller experiments settings}
The dataset includes 55 channels of EEG recordings from 11 subjects. Each subject is presented with 10 repetitions of 60 to 70 sets of 30 different letters and symbols. In total there are approximately 20,000 samples for each subject where 1/30 of them are supposed to contain a P300 wave. While the original experiment contains 3 different settings (interval of 116ms with/without colors and 83ms with color), we used the experiment setting of 116ms intervals with letters in different colors. For more details, see \cite{BlaknertzExperiment}. 

In addition to the filters applied in~\cite{BlaknertzExperiment}, all models that we used share the same pre-processing stage of down-sampling the input frequency from 200Hz to 25 Hz. The result is that each learning sample is a matrix of 55 channels with 25 time samples each, or $55*25 = 1375$ features. Each sample thus covers exactly 1 second around the target event, at times [-200,800] ms.

\subsection{Formulating the BCI task}
In P300 speller the task is to identify which letter the subject paid his attention by identifying a P300 pattern in the EEG. This can be done by finding a function $f(x)$ that when given an EEG sample – $x$, returns the probability that a P300 pattern is found in it. By identifying the EEG sample with the maximal $f(x)$ score among the EEG samples of all the different letters we can identify the target stimulus (i.e., the letter that the subject focused on). First, we will formulate the single letter prediction task.

\subsection{RSVP P300 speller trials}

\paragraph{Notations}
\begin{labeling}{notation}
	\item [$f(x)$] P300 identification function
	\item [$X_c$] An EEG sample data that was recorded when presented character $c$
	\item [$C$] The set of all the available stimuli 
	\item [$\hat{c}$] The predicted target stimuli
	\item [$c*$] The true target stimuli (i.e. the letter the subject was suppose to focus on)	
	\item [$R$] Group of trial
	\item [$x_{c,r}$] An EEG sample data that was recorded when presented character $c$ on trial $r$
\end{labeling}

A \underline{\textit{trial}} is the presentation of all the characters in an alphabet, one after the other, in random order. In each trial, only one letter is the actual target stimuli.  

The predicted character $\hat{c}$ in a single trial is computed by finding the character with maximal value of $f(x)$ among the group of letters ($C$):


\begin{equation}
\hat{c} = \arg \max \left\{ {f\left( {{x_{c}}} \right)} \right\}{  _{c \in C}}
\end{equation}

In order to achieve robust character recognition prediction, a common approach is repeating each trial several times and averaging the prediction for each different stimulus. The set of repetitions of the same trial will be called $R$ . An EEG recording where stimuli $c$ presented in trial $r$ is called $x_{c,r}$. The formula for predicting the target stimuli across $R$ attempts is:
\begin{equation}
\hat c = \arg \max \left\{ {\frac{1}{R}\sum\limits_{r \in R}^{} {f\left( {{x_{c,r}}} \right)} } \right\}{_{c \in C}}
\end{equation}

\subsection{Loss function}
In neural networks, the weights are updated by deriving the loss function with respect to the network weights. If we train the neural network to identify P300 directly (as in \cite{P300_CNN}), the error only depends on whether the sample contains P300 or not. Here $y_{r,c}$ refers to the true label of sample $x_{r,c}$:
\begin{equation}
E = e\left( {f\left( {{x_{r,c}}} \right),{y_{r,c}}} \right)
\end{equation} 

In this research we use the binary log loss function:
\begin{equation}\label{eq:binary_log_loss}
e\left( {f\left( {{x_{r,c}}} \right),{y_{r,c}}} \right) =  - \left( {{y_{r,c}}\log \left( {f\left( {{x_{r,c}}} \right)} \right) - \left( {1 - {y_{r,c}}} \right)\log \left( {1 - f\left( {{x_{r,c}}} \right)} \right)} \right)
\end{equation} 


The error in a neural network is typically calculated on multiple samples. In this case the error is:
\begin{equation}\label{eq:mini_batch_loss}
E = \frac{1}{M}\sum\nolimits_i^M {e\left( {f\left( {{x_i},{y_i}} \right)} \right)}
\end{equation}

Here $M$ indicates the size of the samples batch.


\subsection{Evaluated Models}
The models evaluated in this experiment are:
\begin{itemize}
	\item LDA - A common method used in P300 classification for BCI is linear discriminative analysis \cite{BlaknertzExperiment,P300_Tutorial}. Here we will use a simplified version; unlike \cite{BlaknertzExperiment} we use all the timestamps as features, and we use a non-shrinkage version of LDA.
	
	\item CNN (Fig.\ref{fig:CNN_model} and Fig.\ref{fig:CNN_detailed}) -- The CNN model we use is similar to the one used in \cite{P300_CNN}. The first layer is composed of 10  spatial filters, each of size $55*1$ -- the number of channels. The second layer contains 13 different temporal filters with size of $1*5$. Each one of the temporal filters processes 5 subsequent time stamps without overlapping. The third and fourth layers are simple fully connected layers followed by a single cell with a sigmoid activation function that emits a scalar.
	
	\item LSTM large/small (Fig.\ref{fig:LSTM_model}) -- LSTM large/small are both composed of single LSTM layers with 100 and 30 hidden cells in each, correspondingly. Both models end with a single cell with a sigmoid activation layer that emits a scalar.
	
	\item LSTM-CNN large/small (Fig.\ref{fig:LSTM_model_CNN} and Fig.\ref{fig:LSTM_CNN_detailed}) -- The model has CNN as a first layer (the spatial domain layer) and LSTM as the second layer for the temporal domain. The first convolutional layer is the same as in the CNN model. Unlike the CNN model, the temporal layer is an LSTM layer with 100/30 hidden cells. The last layer contains a single cell a with a sigmoid activation layer that emits a scalar.

\end{itemize}

	\begin{figure*}[t]
		\centering
		\begin{minipage}{.3\textwidth}
			\centering
			\begin{subfigure}{1.0\textwidth}
				\centering
				\includegraphics[height=5cm]{P300_CNN_arch}
				%	\end{center}
				\caption{CNN model}
				\label{fig:CNN_model}
			\end{subfigure}
		\end{minipage}	
		\begin{minipage}{.3\textwidth}
			
			\centering
			\begin{subfigure}{1.0\textwidth}
				\centering
				\includegraphics[height=5cm]{P300_LSTM_arch_v3}
				%	\end{center}
				\caption{LSTM model}
				\label{fig:LSTM_model}
			\end{subfigure}
			
		\end{minipage}%
		\begin{minipage}{.3\textwidth}
			\centering
			\begin{subfigure}{1.0\textwidth}
			\centering
			\includegraphics[height=5cm]{P300_LSTM_CNN_arch_v3}		
			%	\end{center}
			\caption{CNN-LSTM model}
			\label{fig:LSTM_model_CNN}		
			\end{subfigure}
		\end{minipage}
		
		\caption{Schematic diagrams of the neural networks evaluated. FC stands for fully connected layers.}
	\end{figure*}
	
							
								
In order to examine the power of each method in modelling the inter-subject and  intra-subject variance we have conducted the following experiments:
\begin{enumerate}
	\item Training and testing on each subject's data separately in order to explore intra-subject generalization.
	\item Training and testing on different subjects data combined in order to investigate the impact of larger amounts of data.
	\item Training on all subjects expect one. We conduct this experiment in order to explore the value of using a model that was trained off-line, on different subjects, and then use this model on new subject, with or without additional calibration.
\end{enumerate}
									
In addition to the three experiments above, we also examine the model's tolerance to time domain noise. A highly desired property from BCI systems is tolerance to a small degree of noise in the stimuli onset time.  In order to evaluate the resistance to such noise, we use a model trained on the original stimuli onset (i.e, noise level = 0ms) and evaluate its performance on different stimuli onset: noise levels of -120ms,-80ms,-40ms, +40ms, +80ms, and 120ms. We conducted this experiment using 10-fold cross validation in order to be able to get statistically significant results. This last experiment was conducted only on the CNN and LSTM-CNN models and used data from all subjects (as in experiment 2 described above).

For all experiments, the different models were trained using the RMSProp~\cite{tieleman2012lecture} optimizer, for 30 epochs with a learning rate of 0.001 and then continue to train for 30 epochs with the a learning rate of 0.00001.

RMSProp \cite{tieleman2012lecture} is a stochastic gradient descent (SGD) method. Unlike simple SGD, the method can adapt different learning rates for each parameter separately and use a moving average across the past gradient in order to scale the learning rate per feature. We decided to use RMSProp since it is said to be robust and fast \cite{xu2015show, karpathy2015deep, szegedy2016rethinking}.



\begin{figure*}
	\centering
	\begin{subfigure}[t]{1.0\textwidth}
			\centering
	\includegraphics[width=1.1\linewidth]{Figures/CNN_detailed}
	\caption{A diagram of the CNN model used in our research. All the connections are directed towards the next layer.}
	\label{fig:CNN_detailed}
	\end{subfigure}
	\begin{subfigure}[t]{1.0\textwidth}
		\centering
		\includegraphics[width=1.1\linewidth]{Figures/LSTM_CNN_detailed}
		\caption{A diagram of the CNN-LSTM model used in our experiments. Unlike the CNN model, there are connections between cells from the same layer. The inter-layer connection allows the network to model the signal dynamics.}
		\label{fig:LSTM_CNN_detailed}
	\end{subfigure}

	\caption{An additional illustration showing the main differences between the CNN and LSTM-CNN architectures.}
\end{figure*}


\section{Implementation Details}
The code was implemented using the Keras framework \cite{chollet2015keras}. Training was conducted using a 4-core i7 laptop with 16Gb RAM. Training took 110 seconds on 192000 samples for the small LSTM-CNN model and 24 seconds for the CNN model. The considerable difference is due to the distributed nature of  CNN which allows much of the computation to be computed in parallel. Predicting on a single example takes about 0.6 milliseconds. In term of space, both models require less than 70kb of disk space.

One of the advantages of using 'deep learning' models is that they allow compressing knowledge from many samples into a compact form. As we show in our experiments, it is possible to pre-train on multiple subjects and then fine tune to a specific subject's calibration data. For example, training on 3000 calibration samples using the 4-core i7 laptop will take less than a minute (fine-tuning for 30 epochs). After the model is trained, using it for real-time prediction is feasible as well, since predicting each sample takes 0.6 milliseconds.

\chapter{Results}

Tab.\ref{table:AllAverageedResults} summarizes the results of the different experiments; all results are based on an average of 10 consecutive trials to detect the target letter, as in~\cite{BlaknertzExperiment}. The results for training and testing on the same subject (Tab.\ref{table:AccuracyPerSubject}) indicate that LSTM is inferior (82\%), and even the LSTM\_CNN combined model performs less than the the simple LDA method (86 and 93\% in the LSTM\_CNN models and 96\% using LDA) . A possible advantage for LSTM only becomes apparent with larger amounts of data -- when training and testing on all the subjects together (Tab.\ref{table:AllAverageedResults}). The large LSTM model performs poorly -- 77\%; we suspect it is due to the large number of trainable parameters -- 62501 (``over-fitting''); this is why we introduced CNN as a first layer and reduced the number of hidden LSTM cells.

Tab.\ref{table:AccuracyPerSubject} summarizes the results per single subject. When comparing the accuracy result of each subject separately, we can see that there is a significant difference among subjects, across the different models. For example, subject \textit{fat} results in higher accuracy than \textit{icn} regardless of the tested model. Eventually, the best network method - using training on other subjects and recalibration with a combined CNN-LSTM large model, is able to boost the results of the subject with the lowest accuracy to 86\%.

\vspace{0.4cm}	
\begin{table*}[t]
	\captionof{table}{Average accuracy across all experiments.}
	\label{table:AllAverageedResults}
	\centering
	\begin{tabular}{l|ccccc}
		\toprule
		model &  \makecell{number of \\parameter} &  \makecell{accuracy \\
			per subjects} & \makecell{accuracy \\
			all subjects}&\makecell{all but one}&\makecell{all but one \\ after fine\\tuning}\\
		\midrule
		LDA            &            1375 &                   0.96 &                  0.79 &                           0.65 &                                                 x \\
		%MLP            &             148201 &                   0.88 &                     x &                              x &                                                 x \\
		LSTM large       &          62501 &                   0.82 &                  0.77 &                              x &                                                 x \\
		LSTM small     &           10351 &                   0.89 &                   0.9 &                              x &                                                 x \\
		CNN            &            7924 &                   0.98 &                  0.92 &                           0.84 &                                              0.97 \\
		\makecell{LSTM-CNN\\large}&           49041 &                   0.93 &                   0.9 &                              x &                                                 x \\
		\makecell{LSTM-CNN\\small}&            5511 &                   0.86 &                  0.93 &                           0.84 &                                              0.97 \\
		\bottomrule
	\end{tabular}
	
\end{table*}

\begin{table*}
	\centering
	\captionof{table}{Average accuracy per subject.}
	\label{table:AccuracyPerSubject}	
	\begin{tabular}{l|ccccccc}
		\toprule
		{subject} &   LDA &\makecell{LSTM\\large}&  \makecell{LSTM-CNN\\large}&   CNN &\makecell{LSTM\\ small}&\makecell{LSTM-CNN\\small}\\
		\midrule
		fat     &  1.00 &  0.98 &      0.98 &  0.98 &        1.00 &            0.95 \\
		gcb     &  0.91 &  0.82 &      0.88 &  0.92 &        0.74 &            0.75 \\
		gcc     &  1.00 &  0.84 &      0.92 &  1.00 &        0.92 &            0.97 \\
		gcd     &  0.97 &  0.80 &      0.90 &  1.00 &        0.76 &            0.93 \\
		gcf     &  1.00 &  0.92 &      0.94 &  0.95 &        0.97 &            0.95 \\
		gcg     &  0.94 &  0.74 &      0.96 &  0.96 &        0.80 &            0.87 \\
		gch     &  0.97 &  0.93 &      0.96 &  0.97 &        0.97 &            0.96 \\
		iay     &  0.94 &  0.62 &      0.92 &  0.98 &        0.75 &            0.86 \\
		icn     &  0.94 &  0.62 &      0.86 &  0.98 &        0.77 &            0.77 \\
		icr     &  0.93 &  0.97 &      0.98 &  0.98 &        0.98 &            0.98 \\
		pia     &  0.97 &  0.82 &      0.94 &  1.00 &        0.77 &            0.81 \\
		mean    &  0.96 &  0.82 &      0.93 &  0.98 &        0.86 &            0.89 \\
		\bottomrule
	\end{tabular}
	
\end{table*}

In the second stage, we continue to train the model on the rest $3/4$ of the \textbf{test subject's} data using a smaller learning rate (0.0001 using RMSProp) for 30 epochs. The second training stage results are presented in columns \textit{CNN and LSTM-CNN all except one fine tune}. The results indicate that as in the other cross-subject evaluation, the LDA accuracy is much poorer than those of the CNN and LSTM-CNN models (65\% as opposed to 84\%). When we allow calibrating the model for each subject, we achieve an average accuracy of 97\% for both CNN and LSTM-CNN.

\begin{table*}[t]
	\centering
	\captionof{table}{Accuracy when training and testing on different subjects. }
	\label{table:AllExceptOne}	
	\begin{tabular}{l|ccccc}
		\toprule
		{subject} & \makecell{LDA \\all except\\one} &  \makecell{CNN\\all except\\one}&  \makecell{CNN\\all except\\one\\fine tune}& \makecell{SMALL\\LSTM-CNN\\all except\\one}&\makecell{SMALL\\LSTM-CNN\\all except \\one \\ fine tune} \\
		\midrule
		fat     &                  0.94 &                  1.00 &                            1.00 &                             0.98 &                                               1.00 \\
		gcb     &                  0.43 &                  0.83 &                            0.91 &                             0.86 &                                               0.92 \\
		gcc     &                  0.79 &                  0.98 &                            0.98 &                             0.95 &                                               0.97 \\
		gcd     &                  0.66 &                  0.80 &                            0.99 &                             0.83 &                                               0.97 \\
		gcf     &                  0.68 &                  0.89 &                            0.98 &                             0.79 &                                               0.98 \\
		gcg     &                  0.52 &                  0.81 &                            0.94 &                             0.77 &                                               0.90 \\
		gch     &                  0.87 &                  0.97 &                            0.97 &                             0.97 &                                               0.99 \\
		iay     &                  0.48 &                  0.69 &                            0.98 &                             0.67 &                                               0.97 \\
		icn     &                  0.44 &                  0.58 &                            0.92 &                             0.61 &                                               0.95 \\
		icr     &                  0.63 &                  0.81 &                            1.00 &                             0.89 &                                               1.00 \\
		pia     &                  0.77 &                  0.87 &                            0.96 &                             0.91 &                                               0.97 \\
		mean    &                  0.65 &                  0.84 &                            0.97 &                             0.84 &                                               0.97 \\
		\bottomrule
	\end{tabular}
\end{table*}

\vspace{5mm}

Resistance to temporal noise is displayed in Tab.\ref{table:ResistenceToNoise}. LSTM-CNN shows a significant advantage over both LDA and CNN when testing with stimuli onset different than the one used for training. LSTM-CNN-small achieves an accuracy higher by 3\% and 6\% when adding or removing 40ms to the original stimuli onset, and a t-test indicates that the difference between each pair of groups is statistically significant ($p < 0.05$ - marked in bold). LDA accuracy falls by more than 20\% when facing temporal noise.

A possible explanation can be seen when looking at the two network's saliency map (Fig.\ref{fig:t}). In order to investigate the ``attention'', or the sensitivity of the LSTM model, and compare it to the CNN model, we used a technique suggested by~\cite{graves2012supervised} and draw the absolute gradient of the neural network with respect to the input.

If $f(x_{1}, ..., x_{n})$ is a differentiable, scalar-valued function, its gradient is the vector whose components are the $n$ partial derivatives of $f$, which is a vector-valued function. In our case of $f(x|\theta)$ is the neural network with fixed weights $\theta$ and input $x$. The partial derivatives of $f(x|\theta)$ with respect to $x$ can be interpreted as ``how changing each value of $x$ will change the prediction score''. This gradient should not be confused with the gradient used for training, where the goal is to optimize the model parameters $\theta$ when $x$ is fixed.


	In the case of P300 prediction, $x$ is a matrix of $C\times{T}$ ($C$ - number of channels, $T$ - number of time steps) and $f(x|\theta)$ is the neural network where $\theta$ is the model's weights after training. The gradient $\nabla{f(x|\theta)}$ (see Eq.\ref{eq:4}) is a matrix with the same size as the input $x$, where the amplitude of each cell reflects its impact on the function value. Cells with high absolute value can be interpreted as the cells that have a significant influence on the prediction function.
	
	
	\begin{equation}\label{eq:4}
	\nabla f\left( {x|\theta } \right) = \left[ {\begin{array}{*{20}{c}}
		{\frac{{\partial f\left( {x|\theta } \right)}}{{\partial x\left( {{c_1},{t_1}} \right)}}}&{...}&{\frac{{\partial f\left( {x|\theta } \right)}}{{\partial x\left( {{c_1},{t_T}} \right)}}}\\
		{...}&{...}&{...}\\
		{\frac{{\partial f\left( {x|\theta } \right)}}{{\partial x\left( {{c_C},{t_1}} \right)}}}&{...}&{\frac{{\partial f\left( {x|\theta } \right)}}{{\partial x\left( {{c_C},{t_T}} \right)}}}
		\end{array}} \right]
	\end{equation}

\vspace{1cm}
The results displayed in Fig.\ref{fig:CNNSaliencyMap2} and Fig.\ref{fig:LSTMCNNSaliencyMap2} show the average absolute gradient across all \textit{target} samples of a single cross validation test data: the warm colors correspond to high gradient values, indicating that the model is more sensitive to change in this input feature. We can see the sensitivity of the CNN model spread across the recording relatively evenly as opposed to the LSTM-CNN which is focused around the 250ms and 450ms time-stamps.

\begin{small}

	\label{table:ResistenceToNoise}
	\centering
	
	\begin{tabular}{l|ccc}
		\toprule
		{Noise} &  CNN &  LSTM\_CNN &  LDA\\
		\midrule
		-120  &         0.058 &              0.044 &         0.016 \\
		-80   &         0.275 &              0.299 &         0.016 \\
		-40   &         \textbf{0.825} &              \textbf{0.864} &  \textbf{0.565} \\
		40   &         \textbf{0.848} &              \textbf{0.896} &   \textbf{0.608} \\
		80   &         0.335 &              0.390 &         0.260 \\
		120  &         0.042 &              0.042 &         0.059 \\
		\bottomrule
	\end{tabular}
		\captionof{table}{Accuracy when introducing temporal noise. Statistically significant results are highlighted in bold.}
\end{small}

\vspace{0.5cm}

\begin{figure*}
	\centering
	%	\begin{subfigure}{.225\textwidth}
	\begin{subfigure}{.5\textwidth}
		\centering
		\captionsetup{justification=raggedright,
			singlelinecheck=false,
			margin=9em
		}
		\includegraphics[width=0.9\textwidth]{cnn_sailency.png}
		\caption{CNN}
		\label{fig:CNNSaliencyMap2}
	\end{subfigure}\hspace*{-3em}
	%	\begin{subfigure}{.225\textwidth}
	\begin{subfigure}{.5\textwidth}
		\captionsetup{justification=raggedright,
			singlelinecheck=false,
			margin=7em
		}
		\centering
		\includegraphics[width=0.9\textwidth,left]{lstm_cnn_sailency.png}
		\centering
		\caption{LSTM-CNN}
		\label{fig:LSTMCNNSaliencyMap2}
	\end{subfigure}
	\caption{Average gradient across target samples.}
	\label{fig:t}
\end{figure*}


\chapter{Discussion and Future Directions}
In this work we examined the use of LSTM neural networks for the task of BCI P300 speller. Despite its temporal nature, no version of LSTM investigated in this work has shown a significant advantage compared to the CNN model suggested by \cite{P300_CNN}. We did see LSTM results improve with large amounts of data from multiple subjects, and superior results with a combined CNN-LSTM model; moreover, we have shown that this combined model is significantly more robust to temporal noise in the stimuli onset. We also show that the sensitivity of the LSTM based model is much more focused on the area between 250ms to 450ms than CNN based model. This sensitivity is correlated with what we know about the P300 ERP (a peak around 300ms after the stimuli onset). We suggest that the smaller area of sensitivity explains the robustness of the LSTM model to noise in the time domain, since it is less sensitive to the data outside the P300 phenomena. 

In our research we found that the effectiveness of 'deep learning' models is seen as we increase our dataset size. Ideally, one could use a large database with many samples but this may be very expensive and impractical. A possible solution will be to gather existing recordings from different devices, experiments or even tasks. Another solution will be to explore different data augmentation techniques. 


%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------
\renewcommand{\bibsection}{\chapter{References}}


\bibliography{bci_conf}
\bibliographystyle{ieeetrans}

% simple comment
%----------------------------------------------------------------------------------------


\end{document}  

% TODO: conenct the more detail experiments
%@article{Lenet98_debug,
%	author = {LeCun, Yann, et al.},
%	journal = {Proceedings of the IEEE},
%	title = {Gradient-based learning applied to document recognition.},
%	volume = {86},
%	pages = {2278--2324},
%	year = {1998}
%}